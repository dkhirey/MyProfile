%% start of file `template.tex'.
%% Copyright 2006-2013 Xavier Danaux (xdanaux@gmail.com).
%
% This work may be distributed and/or modified under the
% conditions of the LaTeX Project Public License version 1.3c,
% available at http://www.latex-project.org/lppl/.


\documentclass[11pt,a4paper,roman]{moderncv}        % possible options include font size ('10pt', '11pt' and '12pt'), paper size ('a4paper', 'letterpaper', 'a5paper', 'legalpaper', 'executivepaper' and 'landscape') and font family ('sans' and 'roman')

% modern themes
\moderncvstyle{banking}                            % style options are 'casual' (default), 'classic', 'oldstyle' and 'banking'
\moderncvcolor{blue}                                % color options 'blue' (default), 'orange', 'green', 'red', 'purple', 'grey' and 'black'
%\renewcommand{\familydefault}{\sfdefault}         % to set the default font; use '\sfdefault' for the default sans serif font, '\rmdefault' for the default roman one, or any tex font name
\nopagenumbers{}                                  % uncomment to suppress automatic page numbering for CVs longer than one page

% character encoding
\usepackage[utf8]{inputenc}
\usepackage{fontawesome}
\usepackage{fontspec}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{fancyhdr}

\fancyhf{}
%\rhead{Overleaf}
%\lhead{Guides and tutorials}
%\lhead{\fontsize{9pt}{11pt}\selectfont Name}
\lfoot{\fontsize{9pt}{11pt} \color{gray} \selectfont Private Confidential}
\cfoot{\fontsize{9pt}{11pt} \color{gray} \selectfont Page \thepage}
\rfoot{\fontsize{9pt}{11pt} \color{gray} \selectfont Last Updated : June 2021}


% if you are not using xelatex ou lualatex, replace by the encoding you are using
%\usepackage{CJKutf8}                              % if you need to use CJK to typeset your resume in Chinese, Japanese or Korean

% adjust the page margins
\usepackage[scale=0.8]{geometry}
\usepackage{multicol}
%\setlength{\hintscolumnwidth}{3cm}                % if you want to change the width of the column with the dates
%\setlength{\makecvtitlenamewidth}{10cm}           % for the 'classic' style, if you want to force the width allocated to your name and avoid line breaks. be careful though, the length is normally calculated to avoid any overlap with your personal info; use this at your own typographical risks...

\usepackage{import}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    urlcolor=blue
    }

% personal data
\name{Deepak}{Khirey} 


% \title{Curriculum Vitae}                               % optional, remove / comment the line if not wanted
\address{Data Scientist}{Azure Cloud Consultant}{Solution Architect}% optional, remove / comment the line if not wanted; the "postcode city" and and "country" arguments can be omitted or provided empty
% \phone[mobile]{909-839-3097}                   % optional, remove / comment the line if not wanted
% \phone[fixed]{01234 123456}                    % optional, remove / comment the line if not wanted
%\phone[fax]{+3~(456)~789~012}                      % optional, remove / comment the line if not wanted
% \email{xpan1@swarthmore.edu}                               % optional, remove / comment the line if not wanted
% \homepage{shawnpan.me}                         % optional, remove / comment the line if not wanted
% \extrainfo{}                 % optional, remove / comment the line if not wanted
\photo[64pt][0.4pt]{picture}                       % optional, remove / comment the line if not wanted; '64pt' is the height the picture must be resized to, 0.4pt is the thickness of the frame around it (put it to 0pt for no frame) and 'picture' is the name of the picture file
%\quote{Some quote}                                 % optional, remove / comment the line if not wanted

% to show numerical labels in the bibliography (default is to show no labels); only useful if you make citations in your resume
%\makeatletter
%\renewcommand*{\bibliographyitemlabel}{\@biblabel{\arabic{enumiv}}}
%\makeatother
%\renewcommand*{\bibliographyitemlabel}{[\arabic{enumiv}]}% CONSIDER REPLACING THE ABOVE BY THIS

% bibliography with mutiple entries
%\usepackage{multibib}
%\newcites{book,misc}{{Books},{Others}}
  
\newcommand*{\customcventry}[7][.25em]{
  \begin{tabular}{@{}l} 
    {\bfseries #4}
  \end{tabular}
  \hfill% move it to the right
  \begin{tabular}{l@{}}
     {\bfseries #5}
  \end{tabular} \\
  \begin{tabular}{@{}l} 
    {\itshape #3}
  \end{tabular}
  \hfill% move it to the right
  \begin{tabular}{l@{}}
     {\itshape #2}
  \end{tabular}
  \ifx&#7&%
  \else{\\%
    \begin{minipage}{\maincolumnwidth}%
      \small#7%
    \end{minipage}}\fi%
  \par\addvspace{#1}}
  
\newcommand*{\customcvproject}[4][.25em]{
%   \vfill\noindent
  \begin{tabular}{@{}l} 
    {\bfseries #2}
  \end{tabular}
  \hfill% move it to the right
  \begin{tabular}{l@{}}
     {\itshape #3}
  \end{tabular}
  \ifx&#4&%
  \else{\\%
    \begin{minipage}{\maincolumnwidth}%
      \small#4%
    \end{minipage}}\fi%
  \par\addvspace{#1}}

\setlength{\tabcolsep}{12pt}

%----------------------------------------------------------------------------------
%            content
%----------------------------------------------------------------------------------
\begin{document}
%\begin{CJK*}{UTF8}{gbsn}                          % to typeset your resume in Chinese using CJK
%-----       resume       ---------------------------------------------------------
\makecvtitle
\vspace*{-23mm}
\bigbreak{}
\begin{center}
\begin{tabular}{c c }
 \faMobile\enspace \href{tel:+91-845-299-2813}{+91-845-299-2813}  & 
 \faEnvelopeO\enspace \href{mailto:deepakkhirey@gmail.com}{deepakkhirey@gmail.com} & 
 \faGithub\enspace \href{https://github.com/dkhirey}{github/dkhirey} & 
 \faGlobe\enspace \href{https://www.linkedin.com/in/deepakkhirey}{linkedin/deepakkhirey}\\
\end{tabular}
\end{center}

% \hline{}

\section{SUMMARY}
{\begin{itemize}
    \item 16+ years of versatile Industry Experience in Manufacturing - Automotive Domain
    \item Working as Principal Data Scientist since 2+ years in Industrial IoT, Manufacturing Analytics Segment
    \item Experience as Data Architect on Enterprise Cloud Transformation journey with Global Automobile Manufacturer
    \item Experience as Big Data Engineer for 4+ years in Connected Vehicles space for Warranty Analysis and Service Efficiency
    \item Azure Cloud Expert with hands on experience in Design and Deployment of Data Analytics Applications
    \item 10+ years of experience as Solution Architect in Product Design, Product Lifecycle Management area
    \item Experience in Team Building and Mentorship, Customer Relationship Management and Program Management
\end{itemize}}


\section{EDUCATION}
{\customcventry{Graduation: December 2019}{MS in Data Science}{\href{https://www.indiana.edu/}{Indiana University of Bloomington}}{IN, USA}{}{}}
{\customcventry{Graduation: June 2003}{Bachelor of Engineering in Mechanical}{\href{http://www.unipune.ac.in/}{Pune University}}{MH, India}{}{}}

\section{CERTIFICATIONS}
{\begin{itemize}
    \item {\href{https://www.credly.com/badges/a6183143-bff0-41e2-b81c-c2879f50c19d/public_url}{Azure Data Scientist Associate - DP100}}
    \item {\href{https://www.credly.com/badges/e2f12548-835d-4d8f-9355-0a157df89709/public_url}{Azure Fundamentals - AZ900}} 
\end{itemize}}

% \begin{left}
%     \begin{tabular}{l  c  c}
%             {Azure Data Scientist Associate - DP100} \\
%             {\href{https://www.youracclaim.com/badges/e2f12548-835d-4d8f-9355-0a157df89709}{Azure Fundamentals - AZ900}} 
%              & \includegraphics[scale=0.3]{az900.png} & \includegraphics[scale=0.3]{dp100.png}\\
%     \end{tabular}
% \end{left}

% \begin{wrapfigure}{r}{0.25\textwidth}
% \includegraphics[scale=0.25]{dp100.png} 
% % \caption{Caption1}
% % \label{fig:wrapfig}
% \end{wrapfigure}

\section{SKILL SET}
{\begin{itemize}
  \item \textbf{Machine Learning Techniques} - Deep Learning, Timeseries Forecasting, NLP, Computer Vision, Regression, Classification, Clustering, Web Scraping
  \item \textbf{Azure Modules} - Azure Machine Learning, Azure Databricks, Azure HDInsights Azure Data Factory, Azure Functions, Azure Logic Apps, Azure Eventhub, Azure IotHub, Azure Stream Analytics, Azure CosmosDB, Azure Datalake
  \item \textbf{BigData Applications} - Apache Spark, Apache Kafka, Apache Hive, Apache Sqoop, Apache HBase, Apache Oozie, Apache Hadoop 2.7, Apache MapReduce
  \item \textbf{Programming Languages} - Python, Scala, Java, PL/SQL
  \item \textbf{Data Visualization} - Microsoft PowerBI, Tableau, Qlikview
%   \item \textbf{Agile Applications} - Jira, Confluence, GitHub
\end{itemize}
}


\section{WORK EXPERIENCE}


{\customcvproject{\href{https://www.tcs.com/plant-solutions-and-services}{Principal Data Scientist â€“ Plant Solutions Services at TCS}}{January 2020 - Present}
\paragraph{Plant Solutions Services (PSS) organization deals with Manufacturing Analytics initiatives for multiple Customer accounts under Industry 4.0 umbrella. Responsibilities consists of establishing analytics engagement with customer and build project teams with initial hand holding and handover to project account for sustainable growth for further support. This includes -}
    {\begin{itemize}
        \item Responding to presales leads and work with Customer to formulate Business Case based on Functional Inputs and Business Value Assessment.
        \item Carry out pre-engagement activities like Design Thinking, Data Identification and Data Feasibility study, Establishing Hypothesis through multiple stakeholder interactions.
        \item Handle use cases in typical Manufacturing PQIM space - Production (Cycle time Optimization), Quality (Quality Parameters forecasting), Inventory (Replenishment Prediction, Inventory Forecasting), Maintenance (Downtime Prediction, Predictive Maintenance) 
        \item Train Machine Learning model around available data using suitable algorithms.
        \item Presentation to Customer to explain the developed model and its significance in functional terms. 
        \item Identify ways to deploy Analytics application in Customer Application Landscape. Participate in Architecture Review Board meetings alongwith Customer.
        \item Responsible for Design and Deployment of end-to-end Data Pipeline from Data Acquisition to KPI Dashboards.
        \item Hire/Mentor team of Analytics professionals in order to monitor and support deployed application.  

        \end{itemize}
    }
}

\bigbreak

{\customcvproject{ {Data Architect â€“ Advanced Analytics at \href{https://www.fcagroup.com/en-US/pages/home.aspx}{Fiat Chrysler}}}{April 2019 - December 2019}
\paragraph{Fiat Chrysler (FCA) had a On-prem Cluster which was falling short against evergrowing demand of storage and compute for new analytics projects. Participated as a Data Architect in FCA Global Cloud Transformation Initiative to Re-imagine and Design an Azure environment consisting of re-organized Data Flows, Data Lakes, Analytics Applications, Data Governance and Business Dashboards. Program activities include -}
    {\begin{itemize}
        \item Analysis of  Data Sources, Data Variety and Data Consumption Patterns in order to cater to Data Analytics use cases.
        \item Define multi-layered Data Lake Architecture to achieve Data Governance objectives. 
        \item Identify and streamline Data Flow patterns for most common use cases to ensure efficient resource utilization.
        \item Devise strategy for transition from Cloudera on-prem ecosystem to Azure Cloud environment. Promote Cloud adaption for new analytics projects, maintain Hybrid Cloud environment during transition phase and plan sun-downing of on-prem system in a phased manner.
        \item Technical evaluation and Scorecarding of ETL and Data Governance tools available in the market such as Informatica, Talend, Collibra etc to augment new Application landscape. 
        \item Participate in Architecture Review meetings with other stakeholders for Networking, Data Security and other business aspects.
        \end{itemize}
    }
}

\bigbreak

{\customcvproject{ \href{https://www.cummins.com/parts-and-service/digital-products-and-services/connected-diagnostics}{Data Engineer â€“ Advanced Analytics at Cummins Inc.}}{January 2016 - March 2019}
\paragraph{Connected Diagnostics program consists of continuous Fault Code Analysis which uses field data from various Service Stations and Engines of Faults happened in the past and uses Machine Learning algorithms to predict most likelihood of the future Fault occurrences based on Engine heartbeats. This enables Cummins to alert Customers about Health of Engine and possible down times leading to operational efficiency. Tasks involved are -}
  {\begin{itemize}
    \item Develop and Maintain End-to-end Data Pipelines from source to sink
    \item Develop Streaming applications with Azure Eventhub to receive and re-direct data
    \item Build Azure Data Lake to store Engine Heartbeat data
    \item Develop Applications in Scala Programming Language for Data Transformation
    \item Generate Data Quality matrix for Data Consistency
    \item Execute Machine Learning algorithms on acquired data
    \item Develop Dashboards in PowerBI to report analysis results 
    \item Formulate Data Storage and Retention strategy at various Data Stages
  \end{itemize}
  }
}

\section{ACHIEVEMENTS}

{\customcventry{}{\textbf{Intelligent Production Optimizer}}{Unit Finalist at TCS Innovista 2020}{September 2020}{}{}}
\paragraph{Manufacturing Operations are executed on Assembly Line based on Product Order Sequence provided by MES systems. These systems are capable of orchestrating steps to perform but there is no feedback mechanism to learn from historical executions.This innovation gives insights to the Production Manager on Line Balancing and Product Order Mix so that system can self identify and prescribe best sequence of orders that would take least time and improve assembly line productivity. }


\bigbreak

{\customcventry{}{\textbf{Prediction of Logistical Delays of Delivery Trucks based on Real Time Telematics data}}{Winner of CTO Hackathon at Fiat Chrysler}{November 2019}{}{}}

\paragraph{This solution uses Apache Kafka as Streaming platform to ingest data from IoT Telematics REST Service to On-Prem PostgreSQL Data Lake at real time and perform Streaming Analytics with Apache Flink to predict ETA using Google Maps Platform. It enables business to monitor Supply Chain Delivery status and get alerts on possible delays in advance. Developed Custom HTTP Source connector using Confluent Hub as part of this hackathon.}


\section{WORK HISTORY}

% {\customcventry{January 2020 - Present}{Principal Data Scientist}{\href{https://www.tcs.com/}{Tata Consultancy Services}}{Pune, India}{}{}}
% {\customcventry{March 2019 - December 2019}{Data Architect}{\href{https://www.tcs.com/}{Tata Consultancy Services}}{Detroit, MI}{}{}}
{\customcventry{January 2016 - Present}{Data Analytics Team Lead}{\href{https://www.tcs.com/}{Tata Consultancy Services}}{Columbus, IN | Detroit, MI | Pune, India}{}{}}
{\customcventry{December 2010 - December 2015}{PLM Solution Architect}{\href{https://www.tcs.com/}{Tata Consultancy Services}}{Columbus, IN | Pune, India}{}{}}
{\customcventry{March 2010 - November 2010}{Development Team Lead}{\href{https://www.techmahindra.com/en-in/}{Mahindra Satyam Computer Systems}}{Hyderabad, India}{}{}}
{\customcventry{October 2007 - February 2010}{Software Developer}{\href{https://www.kpit.com/}{KPIT Cummins Infosystems}}{Pune, India}{}{}}
{\customcventry{July 2005 - September 2007}{Software Quality Analyst}{\href{https://www.ptc.com/}{Parametric Technology Corporation}}{Pune, India}{}{}}
{\customcventry{May 2004 - June 2005}{CAD Design Engineer}{\href{https://www.mahle.com/}{Mahle Filter Systems}}{Pune, India}{}{}}

% \section{ADDITIONAL}
% {\begin{itemize}
%   \item \textbf{Date of Birth }- September 21, 1981
%   \item \textbf{Base Location }- Pune
% \end{itemize}
% }



% Publications from a BibTeX file without multibib
%  for numerical labels: \renewcommand{\bibliographyitemlabel}{\@biblabel{\arabic{enumiv}}}% CONSIDER MERGING WITH PREAMBLE PART
%  to redefine the heading string ("Publications"): \renewcommand{\refname}{Articles}
\nocite{*}
\bibliographystyle{plain}
\bibliography{publications}                        % 'publications' is the name of a BibTeX file

% Publications from a BibTeX file using the multibib package
%\section{Publications}
%\nocitebook{book1,book2}
%\bibliographystylebook{plain}
%\bibliographybook{publications}                   % 'publications' is the name of a BibTeX file
%\nocitemisc{misc1,misc2,misc3}
%\bibliographystylemisc{plain}
%\bibliographymisc{publications}                   % 'publications' is the name of a BibTeX file

%-----       letter       ---------------------------------------------------------

\end{document}


%% end of file `template.tex'.
